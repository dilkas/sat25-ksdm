\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{forest}
\usepackage{siunitx}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage[inline]{enumitem}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{booktabs}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}{Observation}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}

\forestset{
  sn edges/.style={for tree={edge={-Latex}}}
}

\sisetup{group-separator = {,}}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\expr}{\mathtt{expr}}
\newcommand{\Ctwo}{$\mathsf{C}^{2}$}
\newcommand{\FO}{$\mathsf{FO}$}
\newcommand{\UFO}{$\mathsf{UFO}^{2} + \mathsf{CC}$}
\newcommand{\Cranetwo}{\textsc{Gantry}}
\newcommand{\Cranebfs}{\textsc{Gantry-BFS}}
\newcommand{\Cranegreedy}{\textsc{Gantry-Greedy}}

\newcommand{\crefrangeconjunction}{--}
\crefname{line}{line}{lines}
%\crefname{fact}{Observation}{Observations}
\crefname{assumption}{Assumption}{Assumptions}
\crefalias{enumi}{type}
\crefname{type}{Type}{Types}
\creflabelformat{type}{#2\textup{#1}#3}
\crefalias{enumi}{step}
\crefname{step}{Step}{Steps}
\creflabelformat{step}{#2\textup{#1}#3}
\crefalias{clause}{equation}
\crefname{clause}{Clause}{Clauses}
\creflabelformat{clause}{#2\textup{(#1)}#3}
\crefalias{formula}{equation}
\crefname{formula}{Formula}{Formulas}
\creflabelformat{formula}{#2\textup{(#1)}#3}

\DeclareMathOperator{\CR}{CR}
\DeclareMathOperator{\DR}{DR}
\DeclareMathOperator{\Reff}{Ref}
\DeclareMathOperator{\Doms}{Doms}

\SetKwFunction{CompileWithBaseCases}{CompileWithBaseCases}
\SetKwFunction{Compile}{{\normalfont \textsc{Crane}}}
\SetKwFunction{Propagate}{Propagate}
\SetKwFunction{FindBaseCases}{FindBaseCases}
\SetKwFunction{Simplify}{Simplify}

\pdfinfo{
/TemplateVersion (2025.1)
}

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

\title{Towards Practical First-Order Model Counting}
\author{Anonymous Submission}
\affiliations{}

\begin{document}

\maketitle

\begin{abstract}
  First-order model counting (FOMC) is the problem of counting the number of
  models of a sentence in first-order logic. Since lifted inference techniques
  rely on reductions to variants of FOMC, the design of scalable methods for
  FOMC has attracted attention from both theoreticians and practitioners over
  the past decade. Recently, a new approach based on first-order knowledge
  compilation was proposed. This approach, called \textsc{Crane}, instead of
  simply providing the final count, generates definitions of (possibly
  recursive) functions that can be evaluated with different arguments to compute
  the model count for any domain size. However, this approach is not fully
  automated, as it requires manual evaluation of the constructed functions. The
  primary contribution of this work is a fully automated compilation algorithm,
  called \Cranetwo{}, which transforms the function definitions into C++ code
  equipped with arbitrary-precision arithmetic. These additions allow the new
  FOMC algorithm to scale to domain sizes over \num{500000} times larger than
  the current state of the art, as demonstrated through experimental results.
\end{abstract}

\section{Introduction}

% 1. What is the problem?
% 2. Why is it interesting and important?

\emph{First-order model counting} (FOMC) is the task of determining the number
of models for a sentence in first-order logic over a specified domain. The
weighted variant, WFOMC, computes the total weight of these models, linking
logical reasoning with probabilistic
frameworks~\cite{DBLP:conf/ijcai/BroeckTMDR11}. It builds upon earlier efforts
in weighted model counting for propositional
logic~\cite{DBLP:journals/ai/ChaviraD08} and broader attempts to bridge logic
and
probability~\cite{DBLP:journals/ai/Nilsson86,novak2012mathematical,vsaletic2024graded}.
WFOMC is central to \emph{lifted inference}, which enhances the efficiency of
probabilistic calculations by exploiting
symmetries~\cite{DBLP:conf/ecai/Kersting12}. Lifted inference continues to
advance, with applications extending to constraint satisfaction
problems~\cite{DBLP:journals/jair/TotisDRK23} and probabilistic answer set
programming~\cite{DBLP:journals/ijar/AzzoliniR23}. Moreover, WFOMC has proven
effective at reasoning over probabilistic
databases~\cite{DBLP:journals/debu/GribkoffSB14} and probabilistic logic
programs~\cite{DBLP:journals/ijar/RiguzziBZCL17}. FOMC algorithms have also
facilitated breakthroughs in discovering integer
sequences~\cite{DBLP:conf/ijcai/SvatosJT0K23} and developing recurrence
relations for these sequences~\cite{DBLP:conf/kr/DilkasB23}. Recently, these
algorithms have been extended to perform sampling
tasks~\cite{DBLP:journals/ai/WangPWK24}.

% 3. Why is it hard? (E.g., why do naive approaches fail?)

The complexity of FOMC is generally measured by \emph{data complexity}, with a
formula classified as \emph{liftable} if it can be solved in polynomial time
relative to the domain size~\cite{DBLP:conf/starai/JaegerB12}. While all
formulas with up to two variables are known to be
liftable~\cite{DBLP:conf/nips/Broeck11,DBLP:conf/kr/BroeckMD14},
\citeauthor{DBLP:conf/pods/BeameBGS15}~\shortcite{DBLP:conf/pods/BeameBGS15}
demonstrated that liftability does not extend to all formulas, identifying an
unliftable formula with three variables. Recent work has further extended the
liftable fragment with additional
axioms~\cite{DBLP:conf/aaai/TothK23,DBLP:journals/ai/BremenK23} and counting
quantifiers~\cite{DBLP:journals/jair/Kuzelka21}, expanding our understanding of
liftability.

% 4. Why hasn't it been solved before? (Or, what's wrong with previous proposed
% solutions? How does mine differ?)

FOMC algorithms are diverse, with approaches ranging from \emph{first-order
  knowledge compilation} (FOKC) to local
search~\cite{DBLP:journals/pvldb/NiuRDS11}, Monte Carlo
sampling~\cite{DBLP:journals/cacm/GogateD16}, and anytime
approximation~\cite{DBLP:conf/ijcai/BremenK20}. Among these, FOKC-based
algorithms are particularly prominent, transforming formulas into structured
representations such as circuits or graphs. Notable examples include
\textsc{ForcLift}~\cite{DBLP:conf/ijcai/BroeckTMDR11} and its successor
\textsc{Crane}~\cite{DBLP:conf/kr/DilkasB23}. Another important algorithm,
\textsc{FastWFOMC}~\cite{DBLP:conf/uai/BremenK21}, uses cell enumeration as its
foundation.

% 5. What are the key components of my approach and results? Also include any
% specific limitations.

The \textsc{Crane} algorithm marked a significant step forward, expanding the
range of formulas handled by FOMC algorithms. However, it had notable
limitations:
\begin{enumerate*}[label=(\roman*)]
  \item it required manual evaluation of function definitions to compute model
  counts, and
  \item it introduced recursive functions without proper base cases, making it
  more complex to use.
\end{enumerate*}
To address these shortcomings, we present \Cranetwo{}, a fully automated FOMC
algorithm that overcomes the constraints of its predecessor. \Cranetwo{} can
handle domain sizes over \num{500000} times larger than previous algorithms and
simplifies the user experience by automatically handling base cases and
compiling function definitions into efficient C++ programs.

In \cref{sec:preliminaries}, we cover some preliminaries, and in
\cref{sec:main}, we detail all our technical contributions. Finally, in
\cref{sec:experiments}, we present our experimental results, demonstrating
\Cranetwo{}'s performance compared to other FOMC algorithms, and, in
\cref{sec:conclusion}, we conclude the paper by discussing promising avenues for
future work.

\section{Preliminaries}\label{sec:preliminaries}

In \cref{sec:logic}, we summarise the basic principles of first-order logic.
Then, in \cref{sec:threelogics}, we formally define (W)FOMC and discuss the
distinctions between three variations of first-order logic used for FOMC.\@
Finally, in \cref{sec:algebra}, we introduce the terminology used to describe
the output of the original \textsc{Crane} algorithm, i.e., functions and
equations that define them.

We use $\mathbb{N}_{0}$ to represent the set of non-negative integers. In both
algebra and logic, we write $S\sigma$ to denote the application of a
\emph{substitution} $\sigma$ to an expression $S$, where
$\sigma = [x_{1} \mapsto y_{1}, x_{2} \mapsto y_{2}, \dots, x_{n} \mapsto y_{n}]$
signifies the replacement of all instances of $x_{i}$ with $y_{i}$ for all
$i = 1, \dots, n$.

\subsection{First-Order Logic}\label{sec:logic}

In this section, we will review the basic concepts of first-order logic as they
are used in FOKC algorithms. There are two key differences between the logic
used by these algorithms and the logic supported as input. First,
Skolemization~\cite{DBLP:conf/kr/BroeckMD14} eliminates existential quantifiers
by introducing additional predicates. Please note that Skolemization here
differs from the standard Skolemization procedure that introduces function
symbols~\cite{DBLP:books/daglib/0030198}. Second, the input formula is rewritten
as a conjunction of clauses, each in \emph{prenex normal
  form}~\cite{hinman2018fundamentals}.

A \emph{term} can be either a variable or a constant. An \emph{atom} can be
either
\begin{enumerate*}[label=(\roman*)]
  \item $P(t_{1}, \dots, t_{m})$ for some predicate $P$ and terms
  $t_{1}, \dots, t_{m}$ (written as $P(\mathbf{t})$ for short) or
  \item $x=y$ for some terms $x$ and $y$.
\end{enumerate*}
The \emph{arity} of a predicate is the number of arguments it takes, i.e., $m$
in the case of the predicate $P$ mentioned above. We write $P/m$ to denote a
predicate along with its arity. A \emph{literal} can be either an atom (i.e., a
\emph{positive} literal) or its negation (i.e., a \emph{negative} literal). An
atom is \emph{ground} if it contains no variables, i.e., only constants. A
\emph{clause} is of the form $\forall x_{1} \in \Delta_{1}\text{.
}\forall x_{2} \in \Delta_{2}\dots\text{ }\forall x_{n} \in \Delta_{n}\text{.
}\phi(x_{1}, x_{2}, \dots, x_{n})$, where $\phi$ is a disjunction of literals
that only contain variables $x_{1}, \dots, x_{n}$ (and any constants). We say
that a clause is a \emph{(positive) unit clause} if
\begin{enumerate*}[label=(\roman*)]
  \item there is only one literal with a predicate, and
  \item it is a positive literal.
\end{enumerate*}
Finally, a \emph{formula} is a conjunction of clauses. Throughout the paper, we
will use set-theoretic notation, interpreting a formula as a set of clauses and
a clause as a set of literals.

\subsection{FOMC Algorithms and Their Logics}\label{sec:threelogics}

\begin{table*}[t]
  \centering
  \begin{tabular}{llclll}
    \toprule
    Logic & Sorts & Constants & Variables & Quantifiers & Additional atoms\\
    \midrule
    \FO & one or more & \cmark & unlimited & $\forall$, $\exists$ & $x = y$\\
    \Ctwo & one & \xmark & two & $\forall$, $\exists$, $\exists^{= k}$, $\exists^{\le k}$, $\exists^{\ge k}$ & ---\\
    \UFO & one & \xmark & two & $\forall$ & $|P| = m$\\
    \bottomrule
  \end{tabular}
  \caption[]{A comparison of the three logics used in FOMC based on the following aspects:
    \begin{enumerate*}[label=(\roman*)]
      \item the number of sorts,
      \item support for constants,
      \item the maximum number of variables,
      \item supported quantifiers, and
      \item supported atoms in addition to those of the form $P(\mathbf{t})$ for a predicate $P/n$ and an $n$-tuple of terms $\mathbf{t}$.
    \end{enumerate*}
    Here:
    \begin{enumerate*}[label=(\roman*)]
      \item $k$ and $m$ are non-negative integers, with the latter depending on the domain size,
      \item $P$ represents a predicate, and
      \item $x$ and $y$ are terms.
    \end{enumerate*}
  }\label{tbl:logics}
\end{table*}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

In \cref{tbl:logics}, we outline the differences among three first-order logics
commonly used in FOMC:
\begin{enumerate*}[label=(\roman*)]
  \item \FO{} is the input format for
  \textsc{ForcLift}\footnote{\url{https://github.com/UCLA-StarAI/Forclift}} and
  its extensions
  \textsc{Crane}\footnote{\url{https://doi.org/10.5281/zenodo.8004077}} and
  \Cranetwo{};
  \item \Ctwo{} is often used in the literature on \textsc{FastWFOMC} and
  related
  methods~\cite{DBLP:journals/jair/Kuzelka21,DBLP:conf/aaai/MalhotraS22};
  \item \UFO{} is the input format supported by the most recent implementation
  of
  \textsc{FastWFOMC}\footnote{\url{https://github.com/jan-toth/FastWFOMC.jl}}.
\end{enumerate*}
The notation we use to refer to each logic is standard in the case of \Ctwo{}
and \UFO{}~\cite{tóth2024complexityweightedfirstordermodel} and redefined to be
more specific in the case of \FO{}. All three logics are function-free, and
domains are always assumed to be finite. As usual, we presuppose the
\emph{unique name assumption}, which states that two constants are equal if and
only if they are the same constant~\cite{DBLP:books/aw/RN2020}.

\renewcommand*{\thefootnote}{\arabic{footnote}}

In \FO{}, each term is assigned to a \emph{sort}, and each predicate $P/n$ is
assigned to a sequence of $n$ sorts. Each sort has its corresponding domain.
These assignments to sorts are typically left implicit and can be reconstructed
from the quantifiers. For example, $\forall x,y \in \Delta\text{. }P(x, y)$
implies that variables $x$ and $y$ have the same sort. On the other hand,
$\forall x \in \Delta\text{. }\forall y \in \Gamma\text{. } P(x, y)$ implies
that $x$ and $y$ have different sorts, and it would be improper to write, for
example, $\forall x \in \Delta\text{. }\forall y \in \Gamma\text{.
} P(x, y) \lor x = y$. \FO{} is also the only logic to support constants,
formulas with more than two variables, and the equality predicate. While we do
not explicitly refer to sorts in subsequent sections of this paper, the
many-sorted nature of \FO{} is paramount to the algorithms presented therein.

\begin{remark}
  In the case of \textsc{ForcLift} and its extensions, support for a formula as
  valid input does not imply that the algorithm can compile the formula into a
  circuit or graph suitable for lifted model counting. However, it is known that
  \textsc{ForcLift} compilation is guaranteed to succeed on any \FO{} formula
  without constants and with at most two
  variables~\cite{DBLP:conf/nips/Broeck11,DBLP:conf/kr/BroeckMD14}.
\end{remark}

Compared to \FO{}, \Ctwo{} and \UFO{} lack support for
\begin{enumerate*}[label=(\roman*)]
  \item constants,
  \item the equality predicate,
  \item multiple domains, and
  \item formulas with more than two variables.
\end{enumerate*}
The advantage that \Ctwo{} brings over \FO{} is the inclusion of \emph{counting
  quantifiers}. That is, alongside $\forall$ and $\exists$, \Ctwo{} supports
$\exists^{=k}$, $\exists^{\le k}$, and $\exists^{\ge k}$ for any positive
integer $k$. For example, $\exists^{=1} x\text{. }\phi(x)$ means that there
exists \emph{exactly one} $x$ such that $\phi(x)$, and $\exists^{\le 2} x\text{.
}\phi(x)$ means that there exist \emph{at most two} such $x$. \UFO{}, on the
other hand, does not support any existential quantifiers but instead
incorporates \emph{(equality) cardinality constraints}. For example, $|P| = 3$
constrains all models to have \emph{precisely three positive literals with the
  predicate $P$}.

\begin{definition}[Model]\label{def:model}
  Let $\phi$ be a formula in \FO{}. For each predicate $P/n$ in $\phi$, let
  ${(\Delta_{i}^{P})}_{i=1}^{n}$ be a list of the corresponding domains. Let
  $\sigma$ be a map from the domains of $\phi$ to their interpretations as sets,
  satisfying the following conditions:
  \begin{enumerate*}[label=(\roman*)]
    \item the sets are pairwise disjoint, and
    \item the constants in $\phi$ are included in the corresponding domains.
  \end{enumerate*}
  A \emph{structure} of $\phi$ is a set $M$ of ground literals defined by adding
  to $M$ either $P(\mathbf{t})$ or $\neg P(\mathbf{t})$ for every predicate
  $P/n$ in $\phi$ and $n$-tuple
  $\mathbf{t} \in \prod_{i=1}^{n} \sigma(\Delta_{i}^{P})$. A structure is a
  \emph{model} if it satisfies $\phi$.
\end{definition}

% (In practice, we typically only specify the size of each domain.)

\begin{remark}
  The distinctness of domains is important in two ways. First, in terms of
  expressiveness, a clause such as $\forall x \in \Delta\text{. }P(x, x)$ is
  valid if predicate $P$ is defined over two copies of the same domain and
  invalid otherwise. Second, having more distinct domains makes the problem more
  decomposable for the FOKC algorithm. With distinct domains, the algorithm can
  make assumptions or deductions about, e.g., the first domain of predicate $P$
  without worrying how (or if) they apply to the second domain.
\end{remark}

While this work focuses on FOMC, we still define the weighted variant of the
problem as Skolemization relies on weights even for unweighted FOMC.

\begin{definition}[WFOMC instance]\label{def:instance}
  A \emph{WFOMC instance} comprises:
  \begin{enumerate*}[label=(\roman*)]
    \item a formula $\phi$ in \FO{},
    \item two (rational) \emph{weights} $w^{+}(P)$ and $w^{-}(P)$ assigned to
    each predicate $P$ in $\phi$, and
    \item $\sigma$ as described in \cref{def:model}.
  \end{enumerate*}
  Unless specified otherwise, we assume all weights to be equal to 1.
\end{definition}

\begin{definition}[WFOMC~\cite{DBLP:conf/ijcai/BroeckTMDR11}]
  Given a WFOMC instance $(\phi, w^{+}, w^{-}, \sigma)$ as in
  \cref{def:instance}, the \emph{(symmetric) weighted first-order model count}
  (WFOMC) of $\phi$ is
  \begin{equation}\label{eq:wfomc}
    \sum_{M \models \phi} \prod_{P(\mathbf{t}) \in M} w^{+}(P) \prod_{\neg P(\mathbf{t}) \in M} w^{-}(P),
  \end{equation}
  where the sum is over all models of $\phi$.
\end{definition}

\begin{example}[Counting functions]\label{example:functions}
  To define predicate $P$ as a function from a domain $\Delta$ to itself, in
  \Ctwo{} one would write $\forall x \in \Delta\text{.
  }\exists^{=1} y \in \Delta\text{. }P(x, y)$. In \UFO{}, the same could be
  written as
  \begin{equation}\label[formula]{eq:functions1}
    \begin{gathered}
      (\forall x, y \in \Delta\text{. }S(x) \lor \neg P(x, y)) \land{}\\
      (|P| = |\Delta|),
    \end{gathered}
  \end{equation}
  where $w^{-}(S) = -1$. Although \cref{eq:functions1} has more models compared
  to its counterpart in \Ctwo{}, the negative weight $w^{-}(S) = -1$ makes some
  of the terms in \cref{eq:wfomc} cancel out.

  Equivalently, in \FO{} we would write
  \begin{equation}\label[formula]{eq:fo}
    \begin{gathered}
      (\forall x \in \Gamma\text{. }\exists y \in \Delta\text{. }P(x, y)) \land{}\\
      (\forall x \in \Gamma\text{. }\forall y, z \in \Delta\text{. }P(x, y) \land P(x, z) \Rightarrow y = z).
    \end{gathered}
  \end{equation}
  The first clause asserts that each $x$ must have at least one corresponding
  $y$, while the second statement adds the condition that if $x$ is mapped to
  both $y$ and $z$, then $y$ must equal $z$. It is important to note that
  \cref{eq:fo} is written with two domains instead of just one. However, we can
  still determine the correct number of functions by assuming that the sizes of
  $\Gamma$ and $\Delta$ are equal. This formulation, as observed by
  \citeauthor{DBLP:conf/kr/DilkasB23}~\shortcite{DBLP:conf/kr/DilkasB23}, can
  prove beneficial in enabling FOKC algorithms to find efficient solutions.
\end{example}

\subsection{Algebra}\label{sec:algebra}

We write $\expr{}$ to represent an arbitrary algebraic expression. It is
important to note that some terms have different meanings in algebra and
logic. In algebra, a \emph{constant} refers to a non-negative integer. Likewise,
a \emph{variable} can either be a parameter of a function or a variable
introduced through summation, such as $i$ in the expression
$\sum_{i=1}^{n} \expr$. A (function) \emph{signature} is
$f(x_{1}, \dots, x_{n})$ (or $f(\mathbf{x})$ for short), where $f$ represents an
$n$-ary function, and each $x_{i}$ represents a variable. An \emph{equation} is
$f(\mathbf{x}) = \expr{}$, with $f(\mathbf{x})$ representing a signature.

\begin{definition}[Base case]\label{def:basecase}
  Let $f(\mathbf{x})$ be a function call where each $x_{i}$ is either a constant
  or a variable (note that signatures are included in this definition). Then
  function call $f(\mathbf{y})$ is considered a \emph{base case} of
  $f(\mathbf{x})$ if $f(\mathbf{y}) = f(\mathbf{x})\sigma$, where $\sigma$ is a
  substitution that replaces one or more $x_{i}$ with a constant.
\end{definition}

\section{Technical Contributions}\label{sec:main}

\begin{figure*}[t]
  \centering
  \begin{tikzpicture}
    \node at (-1, 0) (formula) {$\phi$};
    \node[draw,rounded rectangle] at (3, 0) (compilewithbasecases) {\CompileWithBaseCases};
    \node[draw,rounded rectangle] at (9, 0) (compilation) {Compile to C++};

    \node[draw,rounded rectangle,dashed] at (12, 0) (cpp) {C++ code};
    \node at (12, -1) (sizes) {Domain sizes};

    \node at (15, 0) (count) {Model count};

    \node[draw,rounded rectangle] at (3, -2) (findbasecases) {\FindBaseCases};
    \node[draw,rounded rectangle,left = 0.1cm of findbasecases] (crane) {\Compile};
    \node[draw,rounded rectangle,right = 0.1cm of findbasecases] (propagate) {\Propagate};
    \node[draw,rounded rectangle,right = 0.1cm of propagate] (simplify) {\Simplify};

    \node[draw,fit={(compilewithbasecases) (compilation) (crane) (findbasecases) (propagate)},inner ysep=7pt,yshift=5pt] {};
    \node at (0.6, 0.5) {\Cranetwo};

    \draw[-Latex] (formula) -- (compilewithbasecases);
    \draw[-Latex] (compilewithbasecases) -- node[above] {$\mathcal{E}$} (compilation);
    \draw[-Latex] (compilation) -- (cpp);
    \draw[-Latex] (sizes) -- (cpp);
    \draw[-Latex] (cpp) -- (count);

    \draw[-Latex,dashed] (compilewithbasecases) -- node[midway,left] {uses} (crane);
    \draw[-Latex,dashed] (compilewithbasecases) -- node[midway,left] {uses} (findbasecases);
    \draw[-Latex,dashed] (compilewithbasecases) -- node[midway,left] {uses} (propagate);
    \draw[-Latex,dashed] (compilewithbasecases) -- node[midway,right] {uses} (simplify);
  \end{tikzpicture}
  \caption[]{The outline of using \Cranetwo{} to compute the model count of a formula $\phi$. First, the formula is compiled into a set of equations, which are then used to create a C++ program. This program can be executed with different command line arguments to calculate the model count of $\phi$ for different domain sizes. To accomplish this, the \CompileWithBaseCases function employs several components:
    \begin{enumerate*}[label=(\roman*)]
      \item the FOKC algorithm of \textsc{Crane},
      \item a procedure called \FindBaseCases, which identifies a sufficient set of base cases,
      \item a procedure called \Propagate, which constructs a formula corresponding to a given base case, and
      \item algebraic simplification techniques (denoted as \Simplify).
    \end{enumerate*}
  }\label{fig:overview}
\end{figure*}

\Cref{fig:overview} provides an overview of \Cranetwo{}'s workflow.
\Cref{sec:completing} describes the main algorithm for completing the
definitions of recursive functions with a sufficient set of base cases.
\Cref{sec:identifying,sec:propagating} describe subsidiary algorithms for
constructing a set of base cases and their corresponding logical formulas.
\Cref{sec:smoothing} explains the post-processing techniques for ensuring
accurate model counting. Additionally, \cref{sec:cpp} explains the process of
compiling function definitions into C++ code, greatly expanding upon the range
of formulas that could previously be handled by similar
approaches~\cite{DBLP:conf/kr/KazemiP16}.

\subsection{Completing the Definitions of Functions}\label{sec:completing}

Before describing the main contribution of this work, let us review the
essential aspects of FOKC as realised by \textsc{Crane}. The input formula is
compiled into:
\begin{enumerate*}[label=(\roman*)]
  \item set $\mathcal{E}$ of equations,
  \item map $\mathcal{F}$ from function names to formulas, and
  \item map $\mathcal{D}$ from function names and argument indices to domains.
\end{enumerate*}
$\mathcal{E}$ can contain any number of functions, one of which (denoted by $f$)
represents the solution to the FOMC problem. To compute the FOMC for particular
domain sizes, $f$ must be evaluated with those domain sizes as arguments.
$\mathcal{D}$ records this correspondence between function arguments and
domains.

\begin{algorithm}[t]
  \caption{\protect\CompileWithBaseCases{$\phi$}}\label{alg:compilewithbasecases}
  \KwIn{formula $\phi$}
  \KwOut{set $\mathcal{E}$ of equations}
  $(\mathcal{E}, \mathcal{F}, \mathcal{D}) \gets \Compile{$\phi$}$\;\label{line:first}
  $\mathcal{E} \gets \Simplify{$\mathcal{E}$}$\;\label{line:second}
  \ForEach{base case $f(\mathbf{x}) \in \FindBaseCases{$\mathcal{E}$}$}{
    $\psi \gets \mathcal{F}(f)$\;
    \ForEach{index $i$ such that $x_{i} \in \mathbb{N}_{0}$}{\label{line:loop}
      $\psi \gets \Propagate{$\psi$, $\mathcal{D}(f, i)$, $x_i$}$\;
    }
    $\mathcal{E} \gets \mathcal{E} \cup \CompileWithBaseCases{$\psi$}$\;\label{line:final}
  }
\end{algorithm}

\Cref{alg:compilewithbasecases} presents our overall approach for compiling a
formula into equations that include the necessary base cases. To begin, we use
the FOKC algorithm of the original \textsc{Crane} to compile the formula into
the three components: $\mathcal{E}$, $\mathcal{F}$, and $\mathcal{D}$. After
some algebraic simplification, $\mathcal{E}$ is passed to the \FindBaseCases
procedure (see \cref{sec:identifying}). For each base case $f(\mathbf{x})$, we
retrieve the logical formula $\mathcal{F}(f)$ associated with the function name
$f$ and simplify it using the \Propagate procedure (explained in detail in
\cref{sec:propagating}). We do this by iterating over all indices of
$\mathbf{x}$, where $x_{i}$ is a constant, and using \Propagate to simplify
$\psi$ by assuming that domain $\mathcal{D}(f, i)$ has size $x_{i}$. Finally, on
\cref{line:final}, \CompileWithBaseCases recurses on these simplified formulas
and adds the resulting base case equations to $\mathcal{E}$.
\Cref{example:overall} below provides more detail.

\begin{remark}
  Although \CompileWithBaseCases starts with a call to \textsc{Crane}, the
  proposed algorithm is not just a post-processing step for FOKC because
  \cref{alg:compilewithbasecases} is recursive and can issue more calls to
  \textsc{Crane} on various derived formulas.
\end{remark}

\begin{example}[Counting bijections]\label{example:overall}
  Consider the following formula (previously examined by
  \citeauthor{DBLP:conf/kr/DilkasB23}~\shortcite{DBLP:conf/kr/DilkasB23}) that
  defines predicate $P$ as a bijection between two sets $\Gamma$ and $\Delta$:
  \[
    \begin{gathered}
      (\forall x \in \Gamma\text{. }\exists y \in \Delta\text{. }P(x, y))\land{}\\
      (\forall y \in \Delta\text{. }\exists x \in \Gamma\text{. }P(x, y))\land{}\\
      (\forall x \in \Gamma\text{. }\forall y, z \in \Delta\text{. }P(x, y) \land P(x, z) \Rightarrow y = z)\land{}\\
      (\forall x, z \in \Gamma\text{. }\forall y \in \Delta\text{. }P(x, y) \land P(z, y) \Rightarrow x = z).
    \end{gathered}
  \]
  We specifically examine the first solution returned by \Cranetwo{} for this
  formula.

  After \cref{line:first,line:second}, we have
  \begin{align*}
    \mathcal{E} &= \left\{\,\begin{aligned}f(m, n) &= \sum_{l=0}^{n} \binom{n}{l}{(-1)}^{n-l}g(l, m),\\ g(l, m) &= g(l-1, m) + mg(l-1, m-1)\end{aligned}\,\right\};\\
    \mathcal{D} &= \{\, (f, 1) \mapsto \Gamma, (f, 2) \mapsto \Delta, (g, 1) \mapsto \Delta^{\top}, (g, 2) \mapsto \Gamma \,\},
  \end{align*}
  where $\Delta^{\top}$ is a new domain. (We omit the definition of
  $\mathcal{F}$ as the formulas can get a bit verbose.) Then \FindBaseCases
  identifies two base cases: $g(0, m)$ and $g(l, 0)$. In both cases,
  \CompileWithBaseCases recurses on the formula $\mathcal{F}(g)$ simplified by
  assuming that one of the domains is empty. In the first case, we recurse on
  the formula $\forall x \in \Gamma\text{. }S(x) \lor \neg S(x)$, where $S$ is a
  predicate introduced by Skolemization with weights $w^{+}(S) = 1$ and
  $w^{-}(S) = -1$. Hence, we obtain the base case $g(0, m) = 0^{m}$. In the case
  of $g(l, 0)$, \Propagate{$\psi$, $\Gamma$, $0$} returns an empty formula,
  resulting in $g(l, 0) = 1$.
\end{example}

It is worth noting that these base cases overlap when $l = m = 0$ but remain
consistent since $0^{0} = 1$. Generally, let $\phi$ be a formula with two
domains $\Gamma$ and $\Delta$, and let $n, m \in \mathbb{N}_{0}$. Then the FOMC
of \Propagate{$\phi$, $\Delta$, $n$} assuming $|\Gamma| = m$ is the same as the
FOMC of \Propagate{$\phi$, $\Gamma$, $m$} assuming $|\Delta| = n$.

Finally, the main responsibility of the \Simplify procedure is to handle the
algebraic pattern $\sum_{m=0}^{n}[a \le m \le b] f(m)$. Here:
\begin{enumerate*}[label=(\roman*)]
  \item $n$ is a variable,
  \item $a, b \in \mathbb{N}_{0}$ are constants,
  \item $f$ is an expression that may depend on $m$, and
  \item $[a \le m \le b] =
  \begin{cases}
    1 & \text{if $a \le m \le b$} \\
    0 & \text{otherwise}
  \end{cases}$.
\end{enumerate*}
\Simplify transforms this pattern into
$f(a) + f(a+1) + \cdots + f(\min\{\, n, b \,\})$. For instance, in the case of
\cref{example:overall}, \Simplify transforms
$g(l, m) = \sum_{k=0}^{m}[0 \le k \le 1]\binom{m}{k}g(l-1, m-k)$ into
$g(l, m) = g(l-1, m) + mg(l-1, m-1)$.

\subsection{Identifying a Sufficient Set of Base Cases}\label{sec:identifying}

\begin{algorithm}[t]
  \caption{\protect\FindBaseCases{$\mathcal{E}$}}\label{alg:findbasecases}
  \KwIn{set $\mathcal{E}$ of equations}
  \KwOut{set $\mathcal{B}$ of base cases}

  $\mathcal{B} \gets \emptyset$\;
  \ForEach{function call $f(\mathbf{y})$ on the right-hand side of an equation in $\mathcal{E}$}{\label{line:functioncall}
    $\mathbf{x} \gets \text{the parameters of $f$ in its definition}$\;
    \ForEach{$y_{i} \in \mathbf{y}$} {
      \uIf{$y_{i} \in \mathbb{N}_{0}$}{
        $\mathcal{B} \gets \mathcal{B} \cup \{\, f(\mathbf{x})[x_{i} \mapsto y_{i}] \,\}$\;
      }
      \ElseIf{$y_{i} = x_{i} - c_{i}$ for some $c_{i} \in \mathbb{N}_{0}$}{
        \For{$j \gets 0$ \KwTo $c_{i} - 1$}{\label{line:lim}
          $\mathcal{B} \gets \mathcal{B} \cup \{\, f(\mathbf{x})[x_{i} \mapsto j] \,\}$\;\label{line:insert}
        }
      }
    }
  }
\end{algorithm}
% (later, let's not bother yet): replace x_i-c_i on Line 7 of Algorithm 2 with a
% generalisation of x_i that can be an arbitrary summation/subtraction of domain
% sizes (propagating this change to the rest of the paper, particularly
% Assumption 1, the proof of Lemma 1)

\Cref{alg:findbasecases} summarises the implementation of \FindBaseCases.
\FindBaseCases considers two types of arguments when a function $f$ calls itself
recursively:
\begin{enumerate*}[label=(\roman*)]
  \item constants and
  \item arguments of the form $x_{i} - c_{i}$, where $c_{i}$ is a constant and
  $x_{i}$ is the $i$-th argument of the signature of $f$.
\end{enumerate*}
When the argument is a constant $c_{i}$, a base case with $c_{i}$ is added. In
the second case, a base case is added for each constant from $0$ up to (but not
including) $c_{i}$.

\begin{example}
  Consider the recursive function $g$ from \cref{example:overall}.
  \FindBaseCases iterates over two function calls: $g(l-1, m)$ and
  $g(l-1, m-1)$. The former produces the base case $g(0, m)$, while the latter
  produces both $g(0, m)$ and $g(l, 0)$.
\end{example}

It can be shown that the base cases identified by \FindBaseCases are sufficient
for the algorithm to terminate.\footnote{Note that characterising the
  fine-grained complexity of the solutions found by \Cranetwo{} or other FOMC
  algorithms is an emerging area of research. These questions have been
  partially addressed in previous
  work~\cite{DBLP:conf/kr/DilkasB23,tóth2024complexityweightedfirstordermodel}
  and are orthogonal to the goals of this section.}

\begin{theorem}[Termination]\label{thm:halting}
  Let $\mathcal{E}$ represent the equations returned by \CompileWithBaseCases.
  Let $f$ be an $n$-ary function in $\mathcal{E}$ and
  $\mathbf{x} \in \mathbb{N}_{0}^{n}$. Then the evaluation of $f(\mathbf{x})$
  terminates.
\end{theorem}

We prove \cref{thm:halting} using double induction. First, we apply induction to
the number of functions in $\mathcal{E}$. Then, we use induction on the arity of
the `last' function in $\mathcal{E}$ according to some topological ordering. For
the detailed proof, please refer to the technical appendix.

\subsection{Propagating Domain Size Assumptions}\label{sec:propagating}

\begin{algorithm}[t]
  \caption{\protect\Propagate{$\phi$, $\Delta$, $n$}}\label{alg:propagate}
  \KwIn{formula $\phi$, domain $\Delta$, $n \in \mathbb{N}_{0}$}
  \KwOut{formula $\phi'$}
  $\phi' \gets \emptyset$\;
  \uIf{$n = 0$}{
    \ForEach{clause $C \in \phi$}{
      \lIf{$\Delta \not\in \Doms(C)$}{$\phi' \gets \phi' \cup \{\, C \,\}$}
      \Else{
        $C' \gets \{\, l \in C \mid \Delta \not\in \Doms(l) \,\}$\;
        \If{$C' \ne \emptyset$}{\label{line:presmoothing}
          $l \gets \text{an arbitrary literal in } C'$\;\label{line:smoothing1}
          $\phi' \gets \phi' \cup \{\, C' \cup \{\, \neg l \,\} \,\} $\;\label{line:smoothing2}
        }
      }
    }
  }
  \Else{
    $D \gets \text{a set of $n$ new constants in $\Delta$}$\;
    \ForEach{clause $C \in \phi$}{
      ${(x_{i})}_{i=1}^{m} \gets \text{the variables in $C$ with domain $\Delta$}$\;
      \lIf{$m = 0$}{$\phi' \gets \phi' \cup \{\, C \,\}$}
      \Else{
        $\phi' \gets \phi' \cup \{\, C[x_{1} \mapsto c_{1}, \dots, x_{m} \mapsto c_{m}] \mid {(c_{i})}_{i=1}^{m} \in D^{m} \,\}$\;
      }
    }
  }
\end{algorithm}

\Cref{alg:propagate}, called \Propagate, modifies the formula $\phi$ based on
the assumption that $|\Delta| = n$. When $n=0$, some clauses become vacuously
satisfied and can be removed. When $n > 0$, partial grounding is performed by
replacing all variables quantified over $\Delta$ with constants. (None of the
formulas examined in this work had $n > 1$.) \Cref{alg:propagate} handles these
two cases separately. For a literal or a clause $C$, the set of corresponding
domains is denoted as $\Doms(C)$.

In the case of $n = 0$, there are three types of clauses to consider:
\begin{enumerate*}[label=(\roman*)]
  \item those that do not mention $\Delta$,\label[type]{type1}
  \item those in which every literal contains variables quantified over
  $\Delta$, and\label[type]{type2}
  \item those that have some literals with variables quantified over $\Delta$
  and some without.\label[type]{type3}
\end{enumerate*}
Clauses of \cref{type1} are transferred to the new formula $\phi'$ without any
changes. For clauses of \cref{type2}, $C'$ is empty, so these clauses are
filtered out. As for clauses of \cref{type3}, a new kind of smoothing is
performed, which will be explained in \cref{sec:smoothing}.

In the case of $n>0$, $n$ new constants are introduced. Let $C$ be an arbitrary
clause in $\phi$, and let $m \in \mathbb{N}_{0}$ be the number of variables in
$C$ quantified over $\Delta$. If $m=0$, $C$ is added directly to $\phi'$.
Otherwise, a clause is added to $\phi'$ for every possible combination of
replacing the $m$ variables in $C$ with the $n$ new constants.

\begin{example}
  Let $C \equiv \forall x \in \Gamma\text{. }\forall y, z \in \Delta\text{.
  } \neg P(x, y) \lor \neg P(x, z) \lor y=z$. Then
  $\Doms(C) = \Doms(\neg P(x, y)) = \Doms(\neg P(x, z)) = \{\, \Gamma, \Delta \,\}$,
  and $\Doms(y=z) = \{\, \Delta \,\}$. A call to \Propagate{$\{\, C \,\}$,
    $\Delta$, $3$} would result in the following formula with nine clauses:
  \begin{align*}
    (\forall x \in \Gamma\text{. }\neg P(x, c_{1}) \lor& \neg P(x, c_{1}) \lor c_{1}=c_{1})\land{}\\
    (\forall x \in \Gamma\text{. }\neg P(x, c_{1}) \lor& \neg P(x, c_{2}) \lor c_{1}=c_{2})\land{}\\
    \vdots&\\
    (\forall x \in \Gamma\text{. }\neg P(x, c_{3}) \lor& \neg P(x, c_{3}) \lor c_{3}=c_{3}).\\
  \end{align*}
  Here, $c_{1}$, $c_{2}$, and $c_{3}$ are the new constants.
\end{example}

\subsection{Smoothing the Base Cases}\label{sec:smoothing}

\emph{Smoothing} modifies a circuit to reintroduce eliminated atoms, ensuring
the correct model
count~\cite{darwiche2001tractable,DBLP:conf/ijcai/BroeckTMDR11}. In this
section, we describe a similar process performed on
\cref{line:smoothing1,line:smoothing2} of \cref{alg:propagate}.
\Cref{line:presmoothing} checks if smoothing is necessary, and
\cref{line:smoothing1,line:smoothing2} execute it. If the condition on
\cref{line:presmoothing} is not satisfied, the clause is not smoothed but
omitted.

Suppose \Propagate is called with arguments $(\phi, \Delta, 0)$, i.e., we are
simplifying the formula $\phi$ by assuming that the domain $\Delta$ is empty.
Informally, if there is a predicate $P$ in $\phi$ unrelated to $\Delta$,
smoothing preserves all occurrences of $P$ even if all clauses with $P$ become
vacuously satisfied.

\begin{example}\label{example:basecasesmoothing}
  Let $\phi$ be:
  \begin{align}
    (\forall x \in \Delta\text{. }\forall y, z \in \Gamma&\text{. }Q(x) \lor P(y, z))\land{}\label[clause]{eq:example1}\\
    (\forall y, z \in \Gamma'&\text{. }P(y, z))\label[clause]{eq:example2},
  \end{align}
  where $\Gamma' \subseteq \Gamma$ is a domain introduced by a compilation rule.
  It should be noted that $P$, as a relation, is a subset of
  $\Gamma \times \Gamma$.

  Now, let us reason manually about the model count of $\phi$ when
  $\Delta = \emptyset$. Predicate $Q$ can only take one value, $Q = \emptyset$.
  The value of $P$ is fixed over $\Gamma' \times \Gamma'$ by \cref{eq:example2},
  but it can vary freely over
  $(\Gamma \times \Gamma) \setminus (\Gamma' \times \Gamma')$ since
  \cref{eq:example1} is vacuously satisfied by all structures. Therefore, the
  correct FOMC should be $2^{|\Gamma|^2 - |\Gamma'|^2}$. However, without
  \cref{line:smoothing2}, \Propagate would simplify $\phi$ to
  $\forall y, z \in \Gamma'\text{. }P(y, z)$. In this case, $P$ is a subset of
  $\Gamma' \times \Gamma'$. This simplified formula has only one model:
  $\{\, P(y, z) \mid y, z \in \Gamma' \,\}$. By including
  \cref{line:smoothing2}, \Propagate transforms $\phi$ to:
  \begin{gather*}
    (\forall y, z \in \Gamma\text{. }P(y, z) \lor \neg P(y, z))\land{}\\
    (\forall y, z \in \Gamma'\text{. }P(y, z)),
  \end{gather*}
  which retains the correct model count.
\end{example}

It is worth mentioning that the choice of $l$ on \cref{line:smoothing1} of
\cref{alg:propagate} is inconsequential because any choice achieves the same
goal: constructing a tautological clause that retains the literals in $C'$.

\subsection{Generating C++ Code}\label{sec:cpp}

In this section, we will describe the final step of \Cranetwo{} as outlined in
\cref{fig:overview}. This step involves translating the set of equations
$\mathcal{E}$ into C++ code. The resulting C++ program can then be compiled and
executed with different command-line arguments to compute the model count of the
formula for various domain sizes.

Each equation in $\mathcal{E}$ is compiled into a C++ function, along with a
separate cache for memoisation. Let us consider an arbitrary equation
$e = (f(\mathbf{x}) = \expr{}) \in \mathcal{E}$, and let
$\mathbf{c} \in \mathbb{N}_{0}^{n}$ represent the arguments of the corresponding
C++ function. The implementation of $e$ consists of three parts. First, we check
if $\mathbf{c}$ is already present in the cache of $e$. If it is, we simply
return the cached value. Second, for each base case $f(\mathbf{y})$ of
$f(\mathbf{x})$ (as defined in \cref{def:basecase}), we check if $\mathbf{c}$
\emph{matches} $\mathbf{y}$, i.e., $c_{i} = y_{i}$ whenever
$y_{i} \in \mathbb{N}_{0}$. If this condition is satisfied, $\mathbf{c}$ is
redirected to the C++ function that corresponds to the definition of the base
case $f(\mathbf{y})$. Finally, if none of the above cases apply, we evaluate
$\mathbf{c}$ based on the expression $\expr{}$, store the result in the cache,
and return it.

\section{Experimental Evaluation}\label{sec:experiments}

% 1. Introduction
% Objective: Briefly state the goals of the experiments.
% Overview: Summarize the structure of the section.

% 2. Experimental Setup

% Baselines: List and describe the baseline methods or systems used for
% comparison.

Our empirical evaluation sought to compare the runtime performance of {\Cranetwo} with the current state of the art, namely 
\textsc{FastWFOMC} and \textsc{ForcLift}. It is worth remarking that  \textsc{ForcLift} does not support arbitrary precision, and returns error for cases that requires arbitrary precision reasoning. 
Our experiments involve two versions
of \Cranetwo{}: \Cranegreedy{} and \Cranebfs{}. Like its predecessor,
\Cranetwo{} has two modes for applying compilation rules to formulas: one that
uses a greedy search algorithm similar to \textsc{ForcLift} and another that
combines greedy and breadth-first search.

The experiments were conducted using an Intel Skylake \SI{2.4}{\giga\hertz} CPU
with \SI{188}{\gibi\byte} of memory and CentOS~7. C++ programs were compiled
using the Intel C++ Compiler 2020u4. \textsc{FastWFOMC} ran on Julia~1.10.4,
while the other algorithms were executed on the Java Virtual Machine 1.8.0\_201.

% It would not make sense to include the original \textsc{Crane} algorithm in
% the experiments or evaluate \Cranetwo{} without the C++ code generation
% described in \cref{sec:cpp}. Indeed, these algorithms can only produce
% mathematical definitions of functions without evaluating them.

% While greedy search is more efficient (in terms of being able to handle larger
% formulas), it is a heuristic that can miss some solutions.

% Datasets: Provide details about the datasets used in the experiments,
% including size, source, and any preprocessing steps.

\subsection{Benchmarks}
We compare these algorithms using three benchmarks from previous studies. The
first benchmark is the function-counting problem from \cref{example:functions},
previously examined by
\citeauthor{DBLP:conf/kr/DilkasB23}~\shortcite{DBLP:conf/kr/DilkasB23}. The
second benchmark is a variant of the well-known `Friends and Smokers' Markov
logic network~\cite{DBLP:conf/aaai/SinglaD08,DBLP:conf/uai/BroeckCD12}. In
\Ctwo{}, \FO{}, and \UFO{}, this problem can be formulated as
\begin{gather*}
  (\forall x,y \in \Delta\text{. } S(x) \land F(x, y) \Rightarrow S(y)) \land{}\\
  (\forall x \in \Delta\text{. }S(x) \Rightarrow C(x))
\end{gather*}
or, equivalently, in conjunctive normal form as
\begin{gather*}
  (\forall x,y \in \Delta\text{. }S(y) \lor \neg S(x) \lor \neg F(x, y)) \land{}\\
  (\forall x \in \Delta\text{. } C(x) \lor \neg S(x)).
\end{gather*}
Finally, we include the bijection-counting problem previously utilised by
\citeauthor{DBLP:conf/kr/DilkasB23}~\shortcite{DBLP:conf/kr/DilkasB23}. Its
formulation in \FO{} is described in \cref{example:overall}. The equivalent
formula in \Ctwo{} is
\begin{align*}
  (\forall x \in \Delta\text{. }\exists^{=1} y \in \Delta&\text{. }P(x, y))\land{}\\
  (\forall y \in \Delta\text{. }\exists^{=1} x \in \Delta&\text{. }P(x, y)).
\end{align*}
Similarly, in \UFO{} the same formula can be written as
\begin{gather*}
  (\forall x, y \in \Delta\text{. }R(x) \lor \neg P(x, y))\land{}\\
  (\forall x, y \in \Delta\text{. }S(x) \lor \neg P(y, x))\land{}\\
  (|P| = |\Delta|),
\end{gather*}
where $w^{-}(R) = w^{-}(S) = -1$.

\begin{figure*}[t]
  \centering
  \includegraphics{plot.pdf}
  \caption{The runtime of the algorithms as a function of the domain size. Note
    that both axes are on a logarithmic scale.}\label{fig:plot}
\end{figure*}

%Since \textsc{FastWFOMC} does not support many-sorted logic, our experiments are
%limited to formulas with a single domain. However, many of the counting problems
%used in the experimental evaluation of
%\textsc{Crane}~\cite{DBLP:conf/kr/DilkasB23} become equivalent (in terms of
%generating equivalent integer sequences) when restricted to a single domain.
%Additionally, comparing \Cranetwo{} and \textsc{FastWFOMC} on a broader set of
%problems is challenging because each problem needs to be represented in two
%(quite different) logics: \FO{} and \UFO{}.

The three benchmark families cover a wide range of possibilities. The
`friends' benchmark stands out as it uses multiple predicates and can be
expressed in \FO{} using just two variables without cardinality constraints or
counting quantifiers. The `functions' benchmark, on the other hand, can still be
handled by all the algorithms, but it requires cardinality constraints, counting
quantifiers, or more than two variables. Lastly, the `bijections' benchmark is
an example of a formula that \textsc{FastWFOMC} can handle but \textsc{ForcLift}
cannot.

% Metrics: Define the performance metrics used to evaluate the results (e.g.,
% accuracy, precision, recall, F1-score, runtime).

%A considerable difference between \textsc{ForcLift} and the other two algorithms
%is that \textsc{ForcLift} does not support arbitrary precision whereas the other
%algorithms use the GNU Multiple Precision Arithmetic Library. Thus, when the
%count exceeds finite precision, \textsc{ForcLift} returns $\infty$. 

For evaluation purposes, we ran each algorithm on each benchmark using domains
of sizes $2^{1}, 2^{2}, 2^{3}$, and so on, until an algorithm failed to handle a
domain size due to timeout, out of memory error, or out of precision errors.
While we separately measured compilation and inference time, we primarily focus
on total runtime, dominated by the latter.

% Environment: Describe the hardware and software environment (e.g.,
% specifications of the computer, operating system, libraries, and tools used).



% 3. Results and Analysis
% Presentation of Results: Use tables, graphs, and charts to present the experimental results clearly.
% Comparison with Baselines: Compare your method’s results with the baselines.
% Discussion: Analyze and interpret the results, highlighting the strengths and potential weaknesses of your approach.
% Insights: Provide insights or patterns observed from the results.
% Challenges: Discuss any challenges or unexpected outcomes.
% Acknowledgement: Honestly acknowledge any limitations of your experiments or method.

\subsection{Results}
\Cref{fig:plot} presents a summary of the experimental results. Only
\textsc{FastWFOMC} and \Cranebfs{} could handle the bijection-counting problem.
For this benchmark, the largest domain sizes these algorithms could accommodate
were \num{64} and \num{4096}, respectively. On the other two benchmarks,
\textsc{ForcLift} had the lowest runtime. However, due to its finite precision,
it only scaled up to domain sizes of \num{16} and \num{128} for `friends' and
`functions', respectively. \textsc{FastWFOMC} outperformed \textsc{ForcLift} in
the case of `friends', but not `functions', as it could handle domains of size
\num{1024} and \num{64}, respectively. Furthermore, both \Cranebfs{} and
\Cranegreedy{} performed similarly on both benchmarks. Similarly to the
`bijections' benchmark, \Cranetwo{} significantly outperformed the other two
algorithms, scaling up to domains of size \num{8192} and \num{67108864},
respectively.

Another aspect of the experimental results that deserves separate discussion is
compilation. Both Julia and Scala use just-in-time (JIT) compilation, which
means that \textsc{FastWFOMC} and \textsc{ForcLift} take longer to run on the
smallest domain size, where most JIT compilation occurs. In the case of
\Cranetwo{}, it is only run once per benchmark, so the JIT compilation time is
included in its overall runtime across all domain sizes. Additionally, while
\textsc{ForcLift}'s compilation is generally faster than that of \Cranetwo{},
neither significantly affects overall runtime. Specifically, \textsc{ForcLift}
compilation typically takes around \SI{0.5}{\second}, while \Cranetwo{}
compilation takes around \SI{2.3}{\second}.

% 8. Conclusion
% Summary: Briefly summarize the key findings from the experimental results.
% Future Work: Suggest directions for future research based on your findings.

Based on our experiments, which algorithm should be used in practice? If the
formula can be handled by \textsc{ForcLift} and the domain sizes are reasonably
small, \textsc{ForcLift} is likely the fastest algorithm. In other situations,
\Cranetwo{} is expected to be significantly more efficient than
\textsc{FastWFOMC} regardless of domain size, provided both algorithms can
handle the formula.

\section{Conclusion and Future Work}\label{sec:conclusion}

In this work, we have presented a scalable automated FOKC-based approach to
FOMC. Our algorithm involves completing the definitions of recursive functions
and subsequently translating all function definitions into C++ code. Empirical
results demonstrate that \Cranetwo{} can scale to larger domain sizes than
\textsc{FastWFOMC} while supporting a wider range of formulas than
\textsc{ForcLift}. The ability to efficiently handle large domain sizes is
particularly crucial in the weighted setting, as illustrated by the `friends'
example discussed in \cref{sec:experiments}, where the model captures complex
social networks with probabilistic relationships. Without this scalability, the
practical usefulness of these models would be limited.

Future directions for research include conducting a comprehensive experimental
comparison of FOMC algorithms to better understand their comparative performance
across various formulas. The capabilities of \Cranetwo{} could also be
characterised theoretically, e.g.\ by proving completeness for specific logic
fragments like \Ctwo{}. Additionally, the efficiency of FOMC algorithms can be
further analysed using fine-grained complexity, which would provide more
detailed insights into the computational demands of different formulas.

\bibliography{paper}

\section{Reproducibility Checklist}

This paper:
\begin{itemize}
  \item Includes a conceptual outline and/or pseudocode description of AI
        methods introduced (\textbf{yes}/partial/no/NA)
  \item Clearly delineates statements that are opinions, hypothesis, and
        speculation from objective facts and results (\textbf{yes}/no)
  \item Provides well marked pedagogical references for less-familiare readers
        to gain background necessary to replicate the paper (\textbf{yes}/no)
\end{itemize}

Does this paper make theoretical contributions? (\textbf{yes}/no) If yes, please
complete the list below.

\begin{itemize}
  \item All assumptions and restrictions are stated clearly and formally.
        (\textbf{yes}/partial/no)
  \item All novel claims are stated formally (e.g., in theorem statements).
        (\textbf{yes}/partial/no)
  \item Proofs of all novel claims are included. (\textbf{yes}/partial/no)
  \item Proof sketches or intuitions are given for complex and/or novel results.
        (\textbf{yes}/partial/no)
  \item Appropriate citations to theoretical tools used are given.
        (\textbf{yes}/partial/no)
  \item All theoretical claims are demonstrated empirically to hold.
        (\textbf{yes}/partial/no/NA)
  \item All experimental code used to eliminate or disprove claims is included.
        (yes/no/\textbf{NA})
\end{itemize}

Does this paper rely on one or more datasets? (\textbf{yes}/no) If yes, please
complete the list below.

\begin{itemize}
  \item A motivation is given for why the experiments are conducted on the
        selected datasets (\textbf{yes}/partial/no/NA)
  \item All novel datasets introduced in this paper are included in a data
        appendix. (\textbf{yes}/partial/no/NA)
  \item All novel datasets introduced in this paper will be made publicly
        available upon publication of the paper with a license that allows free
        usage for research purposes. (\textbf{yes}/partial/no/NA)
  \item All datasets drawn from the existing literature (potentially including
        authors’ own previously published work) are accompanied by appropriate
        citations. (\textbf{yes}/no/NA)
  \item All datasets drawn from the existing literature (potentially including
        authors’ own previously published work) are publicly available.
        (\textbf{yes}/partial/no/NA)
  \item All datasets that are not publicly available are described in detail,
        with explanation why publicly available alternatives are not
        scientifically satisficing. (\textbf{yes}/partial/no/NA)
\end{itemize}

Does this paper include computational experiments? (\textbf{yes}/no) If yes,
please complete the list below.

\begin{itemize}
  \item Any code required for pre-processing data is included in the appendix.
        (\textbf{yes}/partial/no).
  \item All source code required for conducting and analyzing the experiments is
        included in a code appendix. (\textbf{yes}/partial/no)
  \item All source code required for conducting and analyzing the experiments
        will be made publicly available upon publication of the paper with a
        license that allows free usage for research purposes.
        (\textbf{yes}/partial/no)
  \item All source code implementing new methods have comments detailing the
        implementation, with references to the paper where each step comes from
        (yes/\textbf{partial}/no)
  \item If an algorithm depends on randomness, then the method used for setting
        seeds is described in a way sufficient to allow replication of results.
        (yes/partial/no/\textbf{NA})
  \item This paper specifies the computing infrastructure used for running
        experiments (hardware and software), including GPU/CPU models; amount of
        memory; operating system; names and versions of relevant software
        libraries and frameworks. (\textbf{yes}/partial/no)
  \item This paper formally describes evaluation metrics used and explains the
        motivation for choosing these metrics. (\textbf{yes}/partial/no)
  \item This paper states the number of algorithm runs used to compute each
        reported result. (\textbf{yes}/no)
  \item Analysis of experiments goes beyond single-dimensional summaries of
        performance (e.g., average; median) to include measures of variation,
        confidence, or other distributional information. (\textbf{yes}/no)
  \item The significance of any improvement or decrease in performance is judged
        using appropriate statistical tests (e.g., Wilcoxon signed-rank).
        (\textbf{yes}/partial/no)
  \item This paper lists all final (hyper-)parameters used for each
        model/algorithm in the paper’s experiments. (yes/partial/no/\textbf{NA})
  \item This paper states the number and range of values tried per (hyper-)
        parameter during development of the paper, along with the criterion used
        for selecting the final parameter setting. (yes/partial/no/\textbf{NA})
\end{itemize}

\end{document}
